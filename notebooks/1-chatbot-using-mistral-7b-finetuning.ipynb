{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:01.089651Z","iopub.execute_input":"2025-01-11T08:40:01.089857Z","iopub.status.idle":"2025-01-11T08:40:01.260161Z","shell.execute_reply.started":"2025-01-11T08:40:01.089837Z","shell.execute_reply":"2025-01-11T08:40:01.258652Z"}},"outputs":[{"name":"stdout","text":"Sat Jan 11 08:40:01 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\ncuda_available = torch.cuda.is_available()\nprint(f\"CUDA available: {cuda_available}\")\n\nif cuda_available:\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of GPUs available: {num_gpus}\")\n    \n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n    \n    current_device = torch.cuda.current_device()\n    print(f\"Current GPU device: {current_device}\")\n    print(f\"Current GPU device name: {torch.cuda.get_device_name(current_device)}\")\nelse:\n    print(\"No CUDA-enabled GPU detected.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:01.339992Z","iopub.execute_input":"2025-01-11T08:40:01.340272Z","iopub.status.idle":"2025-01-11T08:40:03.210965Z","shell.execute_reply.started":"2025-01-11T08:40:01.340249Z","shell.execute_reply":"2025-01-11T08:40:03.210054Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nNumber of GPUs available: 1\nGPU 0: Tesla P100-PCIE-16GB\nCurrent GPU device: 0\nCurrent GPU device name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install accelerate peft bitsandbytes git+https://github.com/huggingface/transformers trl py7zr auto-gptq optimum -q\n!pip install unsloth -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\n\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig, TrainingArguments, TrainingArguments, DataCollatorForSeq2Seq\nfrom trl import SFTTrainer, SFTConfig\nfrom unsloth import is_bfloat16_supported","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:05.122025Z","iopub.execute_input":"2025-01-11T08:40:05.122594Z","iopub.status.idle":"2025-01-11T08:40:20.337226Z","shell.execute_reply.started":"2025-01-11T08:40:05.122555Z","shell.execute_reply":"2025-01-11T08:40:20.336212Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n\n#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","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:20.338460Z","iopub.execute_input":"2025-01-11T08:40:20.338791Z","iopub.status.idle":"2025-01-11T08:40:20.358069Z","shell.execute_reply.started":"2025-01-11T08:40:20.338761Z","shell.execute_reply":"2025-01-11T08:40:20.357223Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef6a1ebfffba4bf5ad1659c0083723b7"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Dataset \n\n[Stanford Alpaca: An Instruction-following LLaMA model](https://huggingface.co/datasets/tatsu-lab/alpaca)\n\nDetails on the dataset can be found [here](https://github.com/tatsu-lab/stanford_alpaca)","metadata":{}},{"cell_type":"code","source":"data = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\ndata_df = data.to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:20.359700Z","iopub.execute_input":"2025-01-11T08:40:20.360004Z","iopub.status.idle":"2025-01-11T08:40:22.984933Z","shell.execute_reply.started":"2025-01-11T08:40:20.359974Z","shell.execute_reply":"2025-01-11T08:40:22.983942Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c8e722c33df425d94ef317cb28f67a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(â€¦)-00000-of-00001-a09b74b3ef9c3b56.parquet:   0%|          | 0.00/24.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0eae0a4db840f8a250ece152978d5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53d0897bab8f4fe3930b2d67c0022513"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"data_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:22.986325Z","iopub.execute_input":"2025-01-11T08:40:22.986648Z","iopub.status.idle":"2025-01-11T08:40:23.009450Z","shell.execute_reply.started":"2025-01-11T08:40:22.986619Z","shell.execute_reply":"2025-01-11T08:40:23.008618Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                             instruction  \\\n0                   Give three tips for staying healthy.   \n1                     What are the three primary colors?   \n2                     Describe the structure of an atom.   \n3                       How can we reduce air pollution?   \n4      Describe a time when you had to make a difficu...   \n...                                                  ...   \n51997  Generate an example of what a resume should li...   \n51998  Arrange the items given below in the order to ...   \n51999  Write an introductory paragraph about a famous...   \n52000  Generate a list of five things one should keep...   \n52001  Analyze the given legal document and explain t...   \n\n                                                   input  \\\n0                                                          \n1                                                          \n2                                                          \n3                                                          \n4                                                          \n...                                                  ...   \n51997                                                      \n51998                                   cake, me, eating   \n51999                                     Michelle Obama   \n52000                                                      \n52001  The following is an excerpt from a contract be...   \n\n                                                  output  \\\n0      1.Eat a balanced diet and make sure to include...   \n1      The three primary colors are red, blue, and ye...   \n2      An atom is made up of a nucleus, which contain...   \n3      There are a number of ways to reduce air pollu...   \n4      I had to make a difficult decision when I was ...   \n...                                                  ...   \n51997  Jean Tremaine\\n1234 Main Street, Anytown, CA 9...   \n51998                                     I eating cake.   \n51999  Michelle Obama is an inspirational woman who r...   \n52000  1. Research potential opportunities and carefu...   \n52001  This legal document states that Company A has ...   \n\n                                                    text  \n0      Below is an instruction that describes a task....  \n1      Below is an instruction that describes a task....  \n2      Below is an instruction that describes a task....  \n3      Below is an instruction that describes a task....  \n4      Below is an instruction that describes a task....  \n...                                                  ...  \n51997  Below is an instruction that describes a task....  \n51998  Below is an instruction that describes a task,...  \n51999  Below is an instruction that describes a task,...  \n52000  Below is an instruction that describes a task....  \n52001  Below is an instruction that describes a task,...  \n\n[52002 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>input</th>\n      <th>output</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Give three tips for staying healthy.</td>\n      <td></td>\n      <td>1.Eat a balanced diet and make sure to include...</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What are the three primary colors?</td>\n      <td></td>\n      <td>The three primary colors are red, blue, and ye...</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Describe the structure of an atom.</td>\n      <td></td>\n      <td>An atom is made up of a nucleus, which contain...</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How can we reduce air pollution?</td>\n      <td></td>\n      <td>There are a number of ways to reduce air pollu...</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Describe a time when you had to make a difficu...</td>\n      <td></td>\n      <td>I had to make a difficult decision when I was ...</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>51997</th>\n      <td>Generate an example of what a resume should li...</td>\n      <td></td>\n      <td>Jean Tremaine\\n1234 Main Street, Anytown, CA 9...</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>51998</th>\n      <td>Arrange the items given below in the order to ...</td>\n      <td>cake, me, eating</td>\n      <td>I eating cake.</td>\n      <td>Below is an instruction that describes a task,...</td>\n    </tr>\n    <tr>\n      <th>51999</th>\n      <td>Write an introductory paragraph about a famous...</td>\n      <td>Michelle Obama</td>\n      <td>Michelle Obama is an inspirational woman who r...</td>\n      <td>Below is an instruction that describes a task,...</td>\n    </tr>\n    <tr>\n      <th>52000</th>\n      <td>Generate a list of five things one should keep...</td>\n      <td></td>\n      <td>1. Research potential opportunities and carefu...</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>52001</th>\n      <td>Analyze the given legal document and explain t...</td>\n      <td>The following is an excerpt from a contract be...</td>\n      <td>This legal document states that Company A has ...</td>\n      <td>Below is an instruction that describes a task,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>52002 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"data_df = data_df[:5000]\ndata_df[\"text\"] = data_df[[\"input\", \"instruction\", \"output\"]].apply(lambda x: \"###Human: \" + x[\"instruction\"] + \" \" + x[\"input\"] + \" ###Assistant: \"+ x[\"output\"], axis=1)\ndata = Dataset.from_pandas(data_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:23.010238Z","iopub.execute_input":"2025-01-11T08:40:23.010570Z","iopub.status.idle":"2025-01-11T08:40:23.121522Z","shell.execute_reply.started":"2025-01-11T08:40:23.010537Z","shell.execute_reply":"2025-01-11T08:40:23.120398Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-7-0ea55a7e7ae7>:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data_df[\"text\"] = data_df[[\"input\", \"instruction\", \"output\"]].apply(lambda x: \"###Human: \" + x[\"instruction\"] + \" \" + x[\"input\"] + \" ###Assistant: \"+ x[\"output\"], axis=1)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\")\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:23.122473Z","iopub.execute_input":"2025-01-11T08:40:23.122801Z","iopub.status.idle":"2025-01-11T08:40:24.709743Z","shell.execute_reply.started":"2025-01-11T08:40:23.122775Z","shell.execute_reply":"2025-01-11T08:40:24.708929Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c334b13a8c442b2b68b6b094583526a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58e2a73e135b45a1a86930998ebf4787"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bbea2f6f147441e82587e56f57ad4a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1fe9fb83f9f49ea8ef39943a036941e"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"quantization_config_loading = GPTQConfig(bits=4, disable_exllama=True, tokenizer=tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:24.710757Z","iopub.execute_input":"2025-01-11T08:40:24.710984Z","iopub.status.idle":"2025-01-11T08:40:24.715937Z","shell.execute_reply.started":"2025-01-11T08:40:24.710965Z","shell.execute_reply":"2025-01-11T08:40:24.715094Z"}},"outputs":[{"name":"stderr","text":"Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n                            \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\",\n                            quantization_config=quantization_config_loading,\n                            device_map=\"auto\"\n                        )\n\nmodel.config.use_cache=False\nmodel.config.pretraining_tp=1\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:24.718054Z","iopub.execute_input":"2025-01-11T08:40:24.718259Z","iopub.status.idle":"2025-01-11T08:40:49.808000Z","shell.execute_reply.started":"2025-01-11T08:40:24.718240Z","shell.execute_reply":"2025-01-11T08:40:49.807175Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/963 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4837cf261cab419c9c1b5a5196f78b5d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, grad_output):\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd(cast_inputs=torch.float16)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.16G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d33d88888942c7a0766155203e3f93"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.1-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.14.self_attn.k_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias'}\n- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"649c0bcc06154282929c3a38a19d47df"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"peft_config = LoraConfig(\n    r=16, \n    lora_alpha=16, \n    lora_dropout=0.05, \n    bias=\"none\", \n    task_type=\"CAUSAL_LM\", \n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:49.809344Z","iopub.execute_input":"2025-01-11T08:40:49.809660Z","iopub.status.idle":"2025-01-11T08:40:50.070579Z","shell.execute_reply.started":"2025-01-11T08:40:49.809635Z","shell.execute_reply":"2025-01-11T08:40:50.069551Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset=data,\n    peft_config=peft_config,\n    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n    \n    args = SFTConfig(\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=1,\n        warmup_steps = 5,\n        num_train_epochs = 1, # Set this for 1 full training run.\n        #max_steps = 60,\n        learning_rate = 2e-4,\n        lr_scheduler_type=\"cosine\",\n        save_strategy=\"epoch\",\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        optim = \"paged_adamw_32bit\",\n        weight_decay = 0.01,\n        #lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"mistral-finetuned-alpaca\",\n        dataset_text_field=\"text\",\n        report_to = \"none\",\n        max_seq_length = 512,\n        #dataset_num_proc = 4,\n        packing = False, # Can make training 5x faster for short sequences.\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:50.809450Z","iopub.execute_input":"2025-01-11T08:40:50.809844Z","iopub.status.idle":"2025-01-11T08:40:52.753183Z","shell.execute_reply.started":"2025-01-11T08:40:50.809805Z","shell.execute_reply":"2025-01-11T08:40:52.752556Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4c0efd613204fa3b2d6d73e33386bd0"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# help (SFTTrainer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:52.847588Z","iopub.execute_input":"2025-01-11T08:40:52.847916Z","iopub.status.idle":"2025-01-11T08:40:53.618386Z","shell.execute_reply.started":"2025-01-11T08:40:52.847883Z","shell.execute_reply":"2025-01-11T08:40:53.617265Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:53.619527Z","iopub.execute_input":"2025-01-11T08:40:53.619863Z","iopub.status.idle":"2025-01-11T10:35:51.412398Z","shell.execute_reply.started":"2025-01-11T08:40:53.619831Z","shell.execute_reply":"2025-01-11T10:35:51.411532Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [625/625 1:54:44, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.296800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=625, training_loss=1.278856591796875, metrics={'train_runtime': 6895.0971, 'train_samples_per_second': 0.725, 'train_steps_per_second': 0.091, 'total_flos': 779480250974208.0, 'train_loss': 1.278856591796875, 'epoch': 1.0})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Save the model\noutput_dir = '/kaggle/working/'\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T11:14:13.470411Z","iopub.execute_input":"2025-01-11T11:14:13.470738Z","iopub.status.idle":"2025-01-11T11:14:13.830632Z","shell.execute_reply.started":"2025-01-11T11:14:13.470713Z","shell.execute_reply":"2025-01-11T11:14:13.829645Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/tokenizer_config.json',\n '/kaggle/working/special_tokens_map.json',\n '/kaggle/working/tokenizer.model',\n '/kaggle/working/added_tokens.json',\n '/kaggle/working/tokenizer.json')"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"mistral_finetuned_checkpoints\", 'zip', output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\nfrom transformers import GenerationConfig\nfrom transformers import AutoTokenizer\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\")\n\ninputs = tokenizer(\"\"\"###Human: Why mobile is bad for human? ###Assistant: \"\"\", return_tensors=\"pt\").to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T11:42:35.658018Z","iopub.execute_input":"2025-01-11T11:42:35.658355Z","iopub.status.idle":"2025-01-11T11:42:35.899938Z","shell.execute_reply.started":"2025-01-11T11:42:35.658326Z","shell.execute_reply":"2025-01-11T11:42:35.899172Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"model = AutoPeftModelForCausalLM.from_pretrained(\n    \"/content/mistral-finetuned-alpaca\",\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n                            \"/kaggle/working/mistral-finetuned-alpaca/checkpoint-625\",\n                            quantization_config=quantization_config_loading,\n                            device_map=\"auto\"\n                        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T11:44:49.968112Z","iopub.execute_input":"2025-01-11T11:44:49.968452Z","iopub.status.idle":"2025-01-11T11:45:07.063734Z","shell.execute_reply.started":"2025-01-11T11:44:49.968391Z","shell.execute_reply":"2025-01-11T11:45:07.062772Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.1-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.14.self_attn.k_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias'}\n- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"generation_config = GenerationConfig(\n    do_sample=True,\n    top_k=1,\n    temperature=0.1,\n    max_new_tokens=100,\n    pad_token_id=tokenizer.eos_token_id\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T11:45:07.065041Z","iopub.execute_input":"2025-01-11T11:45:07.065349Z","iopub.status.idle":"2025-01-11T11:45:07.069222Z","shell.execute_reply.started":"2025-01-11T11:45:07.065317Z","shell.execute_reply":"2025-01-11T11:45:07.068343Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import time\n\nst_time = time.time()\noutputs = model.generate(**inputs, generation_config=generation_config)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nprint(time.time()-st_time)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T11:45:23.543785Z","iopub.execute_input":"2025-01-11T11:45:23.544103Z","iopub.status.idle":"2025-01-11T11:47:01.839457Z","shell.execute_reply.started":"2025-01-11T11:45:23.544075Z","shell.execute_reply":"2025-01-11T11:47:01.838591Z"}},"outputs":[{"name":"stdout","text":"###Human: Why mobile is bad for human? ###Assistant: 1. Mobile devices can cause eye strain and headaches due to the small screens and bright lights.\n2. Mobile devices can cause poor posture and lead to back pain.\n3. Mobile devices can cause poor sleep quality and lead to insomnia.\n4. Mobile devices can cause poor memory and lead to forgetfulness.\n5. Mobile devices can cause poor attention span and lead to distraction.\n6. Mobile devices can cause poor social skills and lead to isolation.\n\n98.29079174995422\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"inputs = tokenizer(\"\"\"###Human: Sushi restaurants around me?  ###Assistant: \"\"\", return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, generation_config=generation_config)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nprint(time.time()-st_time)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T11:47:01.840549Z","iopub.execute_input":"2025-01-11T11:47:01.840791Z","iopub.status.idle":"2025-01-11T11:48:39.804438Z","shell.execute_reply.started":"2025-01-11T11:47:01.840764Z","shell.execute_reply":"2025-01-11T11:48:39.803506Z"}},"outputs":[{"name":"stdout","text":"###Human: Sushi restaurants around me?  ###Assistant: 1. Sushi Sensation\n2. Sushi Palace\n3. Sushi Delight\n4. Sushi Express\n5. Sushi Paradise\n6. Sushi Delight\n7. Sushi Delight\n8. Sushi Delight\n9. Sushi Delight\n10. Sushi Delight\n11. Sushi Delight\n12. Sushi Delight\n13\n196.25582885742188\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"inputs = tokenizer(\"\"\"###Human: How to make the best noodles ever? ###Assistant: \"\"\", return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, generation_config=generation_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T11:48:39.806027Z","iopub.execute_input":"2025-01-11T11:48:39.806336Z","iopub.status.idle":"2025-01-11T11:50:17.761254Z","shell.execute_reply.started":"2025-01-11T11:48:39.806311Z","shell.execute_reply":"2025-01-11T11:50:17.760512Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"print(tokenizer.decode(outputs[0], skip_special_tokens=True))\nprint(time.time()-st_time)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T11:50:17.762228Z","iopub.execute_input":"2025-01-11T11:50:17.762490Z","iopub.status.idle":"2025-01-11T11:50:17.767740Z","shell.execute_reply.started":"2025-01-11T11:50:17.762467Z","shell.execute_reply":"2025-01-11T11:50:17.766906Z"}},"outputs":[{"name":"stdout","text":"###Human: How to make the best noodles ever? ###Assistant: 1. Start by boiling water in a large pot.\n2. Add the noodles and cook according to the package instructions.\n3. Drain the noodles and set aside.\n4. In a separate pan, sautÃ© some vegetables such as onions, garlic, and bell peppers.\n5. Add the cooked noodles to the pan and stir until the noodles are heated through.\n6. Add some sauce and stir until the\n294.21908617019653\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = tokenizer(\"\"\"###Human: Sushi restaurants around me and how to get to those places? ###Assistant: \"\"\", return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, generation_config=generation_config)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nprint(time.time()-st_time)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T11:51:03.050400Z","iopub.execute_input":"2025-01-11T11:51:03.050757Z","iopub.status.idle":"2025-01-11T11:52:41.043433Z","shell.execute_reply.started":"2025-01-11T11:51:03.050733Z","shell.execute_reply":"2025-01-11T11:52:41.042716Z"}},"outputs":[{"name":"stdout","text":"###Human: Sushi restaurants around me and how to get to those places? ###Assistant: 1. Sushi Sensation - 123 Main Street, Anytown, USA\n2. Sushi Palace - 456 Elm Street, Anytown, USA\n3. Sushi Delight - 789 Oak Street, Anytown, USA\n4. Sushi Paradise - 101 Pine Street, Anytown, USA\n5. Sushi Bliss - 222 Maple\n437.49526953697205\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}